{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "674dfd6ded4398e0679ff4f65e9a10a54ff0d14801bec0126172cfc3973d1cf1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "\n",
    "df = pd.read_csv(root + os.sep + 'data'+ os.sep + 'BASE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 630844\n",
      "Total words: 118223\n",
      "Unique words before ignoring: 11540\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 5173\n",
      "Number of sequences: 6693\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 20, 5173)          80284960  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 20, 5173)          80284960  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 20, 5173)          80284960  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20, 1)             5174      \n",
      "=================================================================\n",
      "Total params: 240,860,054\n",
      "Trainable params: 240,860,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20, 5173)          26765102  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 20, 5173)          20692     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20, 512)           2649088   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 512)           2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20, 5173)          2653749   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 5173)          20692     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 20, 5173)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose (Conv1DTran (None, 20, 5173)          53525031  \n",
      "=================================================================\n",
      "Total params: 85,636,402\n",
      "Trainable params: 85,614,686\n",
      "Non-trainable params: 21,716\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 20, 5173)          85636402  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 20, 1)             240860054 \n",
      "=================================================================\n",
      "Total params: 326,496,456\n",
      "Trainable params: 85,614,686\n",
      "Non-trainable params: 240,881,770\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(df, maxlen=20, step=3, option='word', model_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:1/66: [DR_acc: 0.269, DF_acc: 0.0] [G loss: 0.691007673740387]\n",
      "0:2/66: [DR_acc: 1.0, DF_acc: 0.9] [G loss: 0.7085055708885193]\n",
      "0:3/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 0.7558914422988892]\n",
      "0:4/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 0.8614627122879028]\n",
      "0:5/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 1.1011285781860352]\n",
      "0:6/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 1.6441595554351807]\n",
      "0:7/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 2.7439534664154053]\n",
      "0:8/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 4.443102836608887]\n",
      "0:9/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 6.588928699493408]\n",
      "0:10/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 8.870881080627441]\n",
      "0:11/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 11.198172569274902]\n",
      "0:12/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 13.217643737792969]\n",
      "0:13/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 14.475693702697754]\n",
      "0:14/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 14.681601524353027]\n",
      "0:15/66: [DR_acc: 1.0, DF_acc: 1.0] [G loss: 14.712143898010254]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dc4a371311fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Documentos\\TheBridge\\bridge_datascience_JorgeGarcia\\Text_Generation_Project\\src\\utils\\models.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval, verbose)\u001b[0m\n\u001b[0;32m    368\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training Discriminator: {n+1}/{batch_per_epoch}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m                 \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m                 \u001b[1;31m#d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1823\u001b[0m                                                     class_weight)\n\u001b[0;32m   1824\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1825\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(epochs=1, batch_size=100, sample_interval=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "gan.generator.save(root + os.sep + 'models' + os.sep + 'Word_Type3_Gan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' hhhhl lor neer l le lolo hiuayl wea n i'"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "sequence = 'certainty of death. small chance of succ'\n",
    "gan.generate(gan.generator, mode='gan', sentence=False, temperature=1)"
   ]
  },
  {
   "source": [
    "### Predict Word-Level Gan"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gan.generator.predict(gan.X[0].reshape(1, gan.X[0].shape[0], gan.X[0].shape[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' the to to okay all to we to me want to to to to in me to have part gotta me to they to me to to me to they to me we to to we to back me to'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "generated = ''\n",
    "for pred in list(preds):\n",
    "    next_index = gan.sample(pred, temperature=1)\n",
    "    next_char = gan.indices_token[next_index]\n",
    "    generated += next_char + ' '\n",
    "generated"
   ]
  },
  {
   "source": [
    "### Predict Character-Level Gan"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gan.generator.predict(gan.X[0].reshape(1, gan.X[0].shape[0], gan.X[0].shape[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\" hhe p'e p i iu weae dmd her l hld hl hh\""
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "generated = ''\n",
    "for pred in list(preds):\n",
    "    next_index = gan.sample(pred, temperature=1)\n",
    "    next_char = gan.indices_token[next_index]\n",
    "    generated += next_char\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(root + os.sep + 'models'+ os.sep + 'Type3_Gan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import Preprocessor\n",
    "\n",
    "class GAN(Preprocessor):\n",
    "\n",
    "    def __init__(self, df, maxlen=40, step=3, option='character', min_word_frequency=2, model_type=1):\n",
    "        Preprocessor.__init__(self, df)\n",
    "        self.maxlen = maxlen\n",
    "        self.step = step\n",
    "\n",
    "        self.preprocess(maxlen=self.maxlen, step=self.step, option=option, mode='gan', min_word_frequency=min_word_frequency)\n",
    "        \n",
    "        optimizer = Adam(learning_rate=0.0002)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator(mode=model_type)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator(mode=model_type)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Build the GAN Model\n",
    "        self.gan = self.build_gan(self.generator, self.discriminator)\n",
    "        self.gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_discriminator(self, mode=1):\n",
    "\n",
    "        if mode == 1:\n",
    "            discriminator = keras.Sequential([\n",
    "                keras.layers.InputLayer(input_shape=(self.maxlen, len(self.tokens))),\n",
    "                layers.LSTM(128),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            discriminator.summary()\n",
    "\n",
    "        elif mode == 2:\n",
    "            discriminator = keras.Sequential([\n",
    "                layers.Conv1D(filters=len(self.tokens),\n",
    "                                    kernel_size=(3),\n",
    "                                    input_shape=(self.maxlen, len(self.tokens)),\n",
    "                                    padding='same'),\n",
    "                layers.Dropout(0.25),\n",
    "                layers.Dense(128),\n",
    "                layers.LeakyReLU(alpha=0.3),\n",
    "                layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            discriminator.summary()\n",
    "\n",
    "        elif mode == 3:\n",
    "            discriminator = keras.Sequential([\n",
    "                layers.Conv1D(filters=len(self.tokens), kernel_size=(3), input_shape=(self.maxlen, len(self.tokens)), padding='same'),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Conv1D(filters=len(self.tokens), kernel_size=(3), input_shape=(self.maxlen, len(self.tokens)), padding='same'),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Conv1D(filters=len(self.tokens), kernel_size=(3), input_shape=(self.maxlen, len(self.tokens)), padding='same'),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            discriminator.summary()\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def build_generator(self, mode=1):\n",
    "        \n",
    "        if mode == 1:\n",
    "            generator = keras.Sequential([\n",
    "                layers.InputLayer(input_shape=(self.maxlen, len(self.tokens))),\n",
    "                layers.LSTM(128, return_sequences=True),\n",
    "                layers.Dense(len(self.tokens), activation='softmax'),\n",
    "            ])\n",
    "            generator.summary()\n",
    "\n",
    "        elif mode == 2:\n",
    "            generator = keras.Sequential([\n",
    "                layers.Conv1D(filters=len(self.tokens),\n",
    "                                    kernel_size=(3),\n",
    "                                    input_shape=(self.maxlen, len(self.tokens)),\n",
    "                                    padding='same'),\n",
    "                layers.Dropout(0.25),\n",
    "                layers.Dense(128),\n",
    "                layers.LeakyReLU(alpha=0.3),\n",
    "                layers.Dense(len(self.chars), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            generator.summary()\n",
    "\n",
    "        elif mode == 3:\n",
    "            generator = keras.Sequential([\n",
    "                layers.Dense((len(self.tokens)), input_shape=(self.maxlen, len(self.tokens))),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.BatchNormalization(momentum=0.8),\n",
    "                layers.Dense(512),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.BatchNormalization(momentum=0.8),\n",
    "                layers.Dense(len(self.tokens)),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.BatchNormalization(momentum=0.8),\n",
    "                layers.Reshape((self.maxlen, len(self.tokens))),\n",
    "                layers.Conv1DTranspose(len(self.tokens), 2, padding=\"same\", activation='softmax')\n",
    "            ])\n",
    "\n",
    "            generator.summary()\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def build_gan(self, g_model, d_model):\n",
    "\n",
    "        gan = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(self.maxlen, len(self.tokens))),\n",
    "            g_model,\n",
    "            d_model\n",
    "        ])\n",
    "        gan.summary()\n",
    "\n",
    "        return gan\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50, verbose=True):\n",
    "\n",
    "        batch_per_epoch = len(self.X)//batch_size\n",
    "        half_batch = batch_size//2\n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for n in range(batch_per_epoch):\n",
    "\n",
    "                # Training the discriminator\n",
    "                # Select a random batch of character sequences\n",
    "                if verbose > 1:\n",
    "                    print(f'Generating real samples: {n+1}/{batch_per_epoch}')\n",
    "                X_real, y_real = self.generate_real_samples(half_batch)\n",
    "\n",
    "                # Generate a batch of fake character sequences\n",
    "                if verbose > 1:\n",
    "                    print(f'Generating fake samples: {n+1}/{batch_per_epoch}')\n",
    "                X_fake, y_fake = self.generate_fake_samples(self.generator, half_batch)\n",
    "\n",
    "                # Train the discriminator\n",
    "                if verbose > 1:\n",
    "                    print(f'Training Discriminator: {n+1}/{batch_per_epoch}')\n",
    "                d_loss_real = self.discriminator.train_on_batch(X_real, y_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(X_fake, y_fake)\n",
    "                #d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                #  Training the Generator\n",
    "                if verbose > 1:\n",
    "                    print(f'Generating GAN samples: {n+1}/{batch_per_epoch}')\n",
    "                X_gan, y_gan = self.generate_gan_samples(batch_size)\n",
    "\n",
    "                # Train the generator (to have the discriminator label samples as real)\n",
    "                if verbose > 1:\n",
    "                    print(f'Training Generator: {n+1}/{batch_per_epoch}')\n",
    "                g_loss = self.gan.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "                # Print the progress and save into loss lists\n",
    "                if epoch % sample_interval == 0 and verbose > 0:\n",
    "                    print(f\"{epoch}:{n+1}/{batch_per_epoch}: [DR_acc: {round(d_loss_real[1],3)}, DF_acc: {round(d_loss_fake[1],3)}] [G loss: {g_loss}]\")\n",
    "                    self.disc_loss.append((d_loss_real, d_loss_fake))\n",
    "                    self.gen_loss.append(g_loss)\n",
    "\n",
    "    def predict(self, option='character', quote_len=40, sentence=False, temperature=1.0, verbose=False):\n",
    "        \n",
    "        if sentence:\n",
    "            if option == 'character':\n",
    "                sentence = sentence[:quote_len]\n",
    "            elif option == 'word':\n",
    "                sentence = sentence.lower()\n",
    "                sentence = sentence.replace('--', ' ')\n",
    "                # split into tokens by white space\n",
    "                text_in_words = sentence.split()\n",
    "                # remove punctuation from each token\n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                text_in_words = [w.translate(table) for w in text_in_words]\n",
    "                pops = [[seq.pop(i) for i, w in enumerate(seq) if w == '' or w == ' '] for seq in text_in_words]\n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                sentence = [word for word in text_in_words if word.isalpha()][:quote_len]\n",
    "\n",
    "        prediction = self.generate(self.generator, mode='gan', option=option, quote_len=quote_len, \n",
    "                                    sentence=sentence, temperature=temperature, verbose=verbose)\n",
    "\n",
    "        return prediction"
   ]
  }
 ]
}