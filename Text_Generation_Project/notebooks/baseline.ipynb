{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "674dfd6ded4398e0679ff4f65e9a10a54ff0d14801bec0126172cfc3973d1cf1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "\n",
    "from src.utils.models import Preprocessor, LSTM_Generator\n",
    "\n",
    "df = pd.read_csv(root + os.sep + 'data'+ os.sep + 'BASE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0              0                         10 things i hate about you   \n",
       "1              1                         10 things i hate about you   \n",
       "2              2                         10 things i hate about you   \n",
       "3              3                         10 things i hate about you   \n",
       "4              4                         10 things i hate about you   \n",
       "...          ...                                                ...   \n",
       "6289        6289  the lord of the rings: the fellowship of the ring   \n",
       "6290        6290      the lord of the rings: the return of the king   \n",
       "6291        6291      the lord of the rings: the return of the king   \n",
       "6292        6292      the lord of the rings: the return of the king   \n",
       "6293        6293      the lord of the rings: the return of the king   \n",
       "\n",
       "                                                  quote  \n",
       "0                           Who knocked up your sister?  \n",
       "1     I was watching you out there, before. I've nev...  \n",
       "2     You're 18, you don't know what you want. And y...  \n",
       "3     Ooh, see that, there. Who needs affection when...  \n",
       "4     Just 'cause you're beautiful, that doesn't mea...  \n",
       "...                                                 ...  \n",
       "6289  If by my life or death I can protect you, I wi...  \n",
       "6290  Certainty of death. Small chance of success. W...  \n",
       "6291  The journey doesn't end here. Death is just an...  \n",
       "6292  I see in your eyes the same fear that would ta...  \n",
       "6293                            But it is not this day!  \n",
       "\n",
       "[6294 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>quote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10 things i hate about you</td>\n      <td>Who knocked up your sister?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10 things i hate about you</td>\n      <td>I was watching you out there, before. I've nev...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10 things i hate about you</td>\n      <td>You're 18, you don't know what you want. And y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10 things i hate about you</td>\n      <td>Ooh, see that, there. Who needs affection when...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>10 things i hate about you</td>\n      <td>Just 'cause you're beautiful, that doesn't mea...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6289</th>\n      <td>6289</td>\n      <td>the lord of the rings: the fellowship of the ring</td>\n      <td>If by my life or death I can protect you, I wi...</td>\n    </tr>\n    <tr>\n      <th>6290</th>\n      <td>6290</td>\n      <td>the lord of the rings: the return of the king</td>\n      <td>Certainty of death. Small chance of success. W...</td>\n    </tr>\n    <tr>\n      <th>6291</th>\n      <td>6291</td>\n      <td>the lord of the rings: the return of the king</td>\n      <td>The journey doesn't end here. Death is just an...</td>\n    </tr>\n    <tr>\n      <th>6292</th>\n      <td>6292</td>\n      <td>the lord of the rings: the return of the king</td>\n      <td>I see in your eyes the same fear that would ta...</td>\n    </tr>\n    <tr>\n      <th>6293</th>\n      <td>6293</td>\n      <td>the lord of the rings: the return of the king</td>\n      <td>But it is not this day!</td>\n    </tr>\n  </tbody>\n</table>\n<p>6294 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root + os.sep + 'data'+ os.sep + 'corpus.txt') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 600000\n",
      "Total chars: 86\n",
      "Number of sequences: 199987\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               110080    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 86)                11094     \n",
      "=================================================================\n",
      "Total params: 121,174\n",
      "Trainable params: 121,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Generator(corpus[:600000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 630844\nTotal chars: 52\nNumber of sequences: 210268\n"
     ]
    }
   ],
   "source": [
    "p = Preprocessor(df)\n",
    "p.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(cp.maxlen, len(cp.chars))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(cp.chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "1643/1643 [==============================] - 82s 49ms/step - loss: 1.9577\n",
      "Epoch 2/40\n",
      "1643/1643 [==============================] - 76s 46ms/step - loss: 1.6627\n",
      "Epoch 3/40\n",
      "1643/1643 [==============================] - 79s 48ms/step - loss: 1.5851\n",
      "Epoch 4/40\n",
      "1643/1643 [==============================] - 83s 50ms/step - loss: 1.5425\n",
      "Epoch 5/40\n",
      "1643/1643 [==============================] - 78s 48ms/step - loss: 1.5142\n",
      "Epoch 6/40\n",
      "1643/1643 [==============================] - 80s 49ms/step - loss: 1.4930\n",
      "Epoch 7/40\n",
      "1643/1643 [==============================] - 81s 49ms/step - loss: 1.4758\n",
      "Epoch 8/40\n",
      "1643/1643 [==============================] - 81s 50ms/step - loss: 1.4631\n",
      "Epoch 9/40\n",
      "1643/1643 [==============================] - 82s 50ms/step - loss: 1.4523\n",
      "Epoch 10/40\n",
      "1643/1643 [==============================] - 84s 51ms/step - loss: 1.4425\n",
      "Epoch 11/40\n",
      "1643/1643 [==============================] - 86s 52ms/step - loss: 1.4353\n",
      "Epoch 12/40\n",
      "1643/1643 [==============================] - 70s 42ms/step - loss: 1.4265\n",
      "Epoch 13/40\n",
      "1643/1643 [==============================] - 68s 41ms/step - loss: 1.4230\n",
      "Epoch 14/40\n",
      "1643/1643 [==============================] - 68s 41ms/step - loss: 1.4164\n",
      "Epoch 15/40\n",
      "1643/1643 [==============================] - 68s 41ms/step - loss: 1.4114\n",
      "Epoch 16/40\n",
      "1643/1643 [==============================] - 67s 41ms/step - loss: 1.4042\n",
      "Epoch 17/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.4013\n",
      "Epoch 18/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3964\n",
      "Epoch 19/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3942\n",
      "Epoch 20/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3894\n",
      "Epoch 21/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3860\n",
      "Epoch 22/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3826\n",
      "Epoch 23/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3811\n",
      "Epoch 24/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3767\n",
      "Epoch 25/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3750\n",
      "Epoch 26/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3732\n",
      "Epoch 27/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3694\n",
      "Epoch 28/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3680\n",
      "Epoch 29/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3660\n",
      "Epoch 30/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3629\n",
      "Epoch 31/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3610\n",
      "Epoch 32/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3578\n",
      "Epoch 33/40\n",
      "1643/1643 [==============================] - 66s 40ms/step - loss: 1.3547\n",
      "Epoch 34/40\n",
      "1643/1643 [==============================] - 70s 43ms/step - loss: 1.3534\n",
      "Epoch 35/40\n",
      "1643/1643 [==============================] - 70s 42ms/step - loss: 1.3519\n",
      "Epoch 36/40\n",
      "1643/1643 [==============================] - 70s 42ms/step - loss: 1.3489\n",
      "Epoch 37/40\n",
      "1643/1643 [==============================] - 71s 43ms/step - loss: 1.3477\n",
      "Epoch 38/40\n",
      "1643/1643 [==============================] - 68s 41ms/step - loss: 1.3447\n",
      "Epoch 39/40\n",
      "1643/1643 [==============================] - 65s 40ms/step - loss: 1.3458\n",
      "Epoch 40/40\n",
      "1643/1643 [==============================] - 69s 42ms/step - loss: 1.3436\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff5d078248>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "model.fit(cp.X, cp.Y, batch_size=batch_size, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ession of the president of the started t'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "sequence = 'certainty of death. small chance of succ'\n",
    "cp.generate(model, temperature=0.2, sentence=sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(root + os.sep + 'models/Base_Quote_Generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 630844\n",
      "Total words: 118223\n",
      "Unique words before ignoring: 11540\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 5173\n",
      "Number of sequences: 28970\n"
     ]
    }
   ],
   "source": [
    "cp = Preprocessor(df)\n",
    "cp.preprocess(maxlen=5, option='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28970, 5, 5173)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "cp.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(cp.maxlen, len(cp.tokens))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(cp.tokens), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "word_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, 128)               2714624   \n_________________________________________________________________\ndense_1 (Dense)              (None, 5173)              667317    \n=================================================================\nTotal params: 3,381,941\nTrainable params: 3,381,941\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "227/227 [==============================] - 27s 112ms/step - loss: 6.3679\n",
      "Epoch 2/100\n",
      "227/227 [==============================] - 25s 110ms/step - loss: 5.7528\n",
      "Epoch 3/100\n",
      "227/227 [==============================] - 25s 110ms/step - loss: 5.3173\n",
      "Epoch 4/100\n",
      "227/227 [==============================] - 26s 113ms/step - loss: 4.8622\n",
      "Epoch 5/100\n",
      "227/227 [==============================] - 24s 106ms/step - loss: 4.4429\n",
      "Epoch 6/100\n",
      "227/227 [==============================] - 23s 101ms/step - loss: 4.0802\n",
      "Epoch 7/100\n",
      "227/227 [==============================] - 23s 101ms/step - loss: 3.6946\n",
      "Epoch 8/100\n",
      "227/227 [==============================] - 23s 101ms/step - loss: 3.3225\n",
      "Epoch 9/100\n",
      "227/227 [==============================] - 23s 102ms/step - loss: 2.9886\n",
      "Epoch 10/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 2.7122\n",
      "Epoch 11/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 2.4848\n",
      "Epoch 12/100\n",
      "227/227 [==============================] - 24s 106ms/step - loss: 2.3371\n",
      "Epoch 13/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 2.2102\n",
      "Epoch 14/100\n",
      "227/227 [==============================] - 25s 108ms/step - loss: 2.1117\n",
      "Epoch 15/100\n",
      "227/227 [==============================] - 25s 108ms/step - loss: 2.0378\n",
      "Epoch 16/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 1.9785\n",
      "Epoch 17/100\n",
      "227/227 [==============================] - 25s 109ms/step - loss: 1.9265\n",
      "Epoch 18/100\n",
      "227/227 [==============================] - 25s 111ms/step - loss: 1.9056\n",
      "Epoch 19/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.8658\n",
      "Epoch 20/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.8379\n",
      "Epoch 21/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.8291\n",
      "Epoch 22/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.8008\n",
      "Epoch 23/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.7819\n",
      "Epoch 24/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.7575\n",
      "Epoch 25/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.7642\n",
      "Epoch 26/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.7360\n",
      "Epoch 27/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.7076\n",
      "Epoch 28/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.7056\n",
      "Epoch 29/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.6960\n",
      "Epoch 30/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.6814\n",
      "Epoch 31/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.6604\n",
      "Epoch 32/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.6574\n",
      "Epoch 33/100\n",
      "227/227 [==============================] - 22s 98ms/step - loss: 1.6523\n",
      "Epoch 34/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.6340\n",
      "Epoch 35/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.6246\n",
      "Epoch 36/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.6147\n",
      "Epoch 37/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.6025\n",
      "Epoch 38/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.5918\n",
      "Epoch 39/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.5828\n",
      "Epoch 40/100\n",
      "227/227 [==============================] - 22s 98ms/step - loss: 1.5806\n",
      "Epoch 41/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.5638\n",
      "Epoch 42/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.5626\n",
      "Epoch 43/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.5607\n",
      "Epoch 44/100\n",
      "227/227 [==============================] - 22s 96ms/step - loss: 1.5499\n",
      "Epoch 45/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.5449\n",
      "Epoch 46/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.5345\n",
      "Epoch 47/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.5268\n",
      "Epoch 48/100\n",
      "227/227 [==============================] - 21s 94ms/step - loss: 1.5311\n",
      "Epoch 49/100\n",
      "227/227 [==============================] - 21s 94ms/step - loss: 1.5258\n",
      "Epoch 50/100\n",
      "227/227 [==============================] - 22s 97ms/step - loss: 1.5222\n",
      "Epoch 51/100\n",
      "227/227 [==============================] - 23s 100ms/step - loss: 1.5054\n",
      "Epoch 52/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.5100\n",
      "Epoch 53/100\n",
      "227/227 [==============================] - 22s 99ms/step - loss: 1.4966\n",
      "Epoch 54/100\n",
      "227/227 [==============================] - 23s 99ms/step - loss: 1.4902\n",
      "Epoch 55/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4933\n",
      "Epoch 56/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 1.4857\n",
      "Epoch 57/100\n",
      "227/227 [==============================] - 24s 105ms/step - loss: 1.4794\n",
      "Epoch 58/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4801\n",
      "Epoch 59/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4786\n",
      "Epoch 60/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4785\n",
      "Epoch 61/100\n",
      "227/227 [==============================] - 24s 105ms/step - loss: 1.4656\n",
      "Epoch 62/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4724\n",
      "Epoch 63/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4661\n",
      "Epoch 64/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4553\n",
      "Epoch 65/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4589\n",
      "Epoch 66/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4487\n",
      "Epoch 67/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 1.4537\n",
      "Epoch 68/100\n",
      "227/227 [==============================] - 24s 107ms/step - loss: 1.4525\n",
      "Epoch 69/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4472\n",
      "Epoch 70/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4469\n",
      "Epoch 71/100\n",
      "227/227 [==============================] - 24s 105ms/step - loss: 1.4367\n",
      "Epoch 72/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4314\n",
      "Epoch 73/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4350\n",
      "Epoch 74/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4305\n",
      "Epoch 75/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4282\n",
      "Epoch 76/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4347\n",
      "Epoch 77/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4230\n",
      "Epoch 78/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4154\n",
      "Epoch 79/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4243\n",
      "Epoch 80/100\n",
      "227/227 [==============================] - 24s 105ms/step - loss: 1.4199\n",
      "Epoch 81/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4134\n",
      "Epoch 82/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4172\n",
      "Epoch 83/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4157\n",
      "Epoch 84/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4071\n",
      "Epoch 85/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4100\n",
      "Epoch 86/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4089\n",
      "Epoch 87/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4091\n",
      "Epoch 88/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.4051\n",
      "Epoch 89/100\n",
      "227/227 [==============================] - 23s 104ms/step - loss: 1.4122\n",
      "Epoch 90/100\n",
      "227/227 [==============================] - 24s 104ms/step - loss: 1.3979\n",
      "Epoch 91/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4013\n",
      "Epoch 92/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.4012\n",
      "Epoch 93/100\n",
      "227/227 [==============================] - 23s 103ms/step - loss: 1.3973\n",
      "Epoch 94/100\n",
      "227/227 [==============================] - 23s 102ms/step - loss: 1.4001\n",
      "Epoch 95/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.3919\n",
      "Epoch 96/100\n",
      "227/227 [==============================] - 21s 94ms/step - loss: 1.3898\n",
      "Epoch 97/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.3992\n",
      "Epoch 98/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.3925\n",
      "Epoch 99/100\n",
      "227/227 [==============================] - 21s 93ms/step - loss: 1.3865\n",
      "Epoch 100/100\n",
      "227/227 [==============================] - 21s 94ms/step - loss: 1.3782\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bcbdf00148>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "word_model.fit(cp.X, cp.Y, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = word_model.predict(cp.X[0].reshape(1, cp.X[0].shape[0], cp.X[0].shape[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5173,)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.save(root + os.sep + 'models' + os.sep + 'Word_Base_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hard moments the i fifty i certain food gone but '"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "generated = ''\n",
    "sentence = 'certainty of death small chance'\n",
    "sentence = sentence.split()\n",
    "\n",
    "for i in range(10):\n",
    "    x_pred = np.zeros((1, cp.maxlen, len(cp.tokens)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, cp.token_indices[char]] = 1.0 #One-Hot Array\n",
    "    preds = word_model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = cp.sample(preds, temperature=1)\n",
    "    next_char = cp.indices_token[next_index]\n",
    "    sentence = sentence[1:]\n",
    "    sentence.append(next_char)\n",
    "    generated += next_char + ' '\n",
    "\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['of', 'death', 'small', 'chance', 'of']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sentence = 'certainty of death small chance'.split()\n",
    "sentence = sentence[1:]\n",
    "sentence.append('of')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(root + os.sep + 'models'+ os.sep + 'Base_Quote_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"e a prosperast. i see the sound of the states the park of the other man was the story. i should be a beat in the fuckin' beautiful the good hall fucking man was a guy. and the brain our or when you go to the path of the country, the started to be the same world. the second in the world, they think this is a good more there was a way the most things with the signs in the rest of the problem with the world. there's no moment of the started the world. i was a little distites of something that i have a world, the said the children. i have a fear and the country are like a bitch in the world. i said the started in the time in the world, and the start of the world. the butt? i would be a little but i see you think i was a brother are because the way the president. i was a secret the more than the park of the dark the country. i see you will be a great community of the troubless of the bast friends of the most month. the day you want to be a little father to the more than a start in the state\""
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "p.generate(model, quote_len=1000, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ess out of the man will find the world. i love the cheat off the second the second in the last times'"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "p.generate(model, quote_len=100, temperature=0.3, sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "gan_model = keras.models.load_model(root + os.sep + 'models'+ os.sep + 'Type3_Gan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 630844\nTotal chars: 52\nNumber of sequences: 210255\n"
     ]
    }
   ],
   "source": [
    "p.preprocess(mode='gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pssr ssssen ssse sen apsen apse sauipsen'"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "p.generate(gan_model, mode='gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "sentence = 'certainty of death. small chance of succ'\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "len('ess out of the man will find the world. i love the cheat off the second the second in the last times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "len('e sssawu aufosse sssrawpltssrawu e e apa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import LSTM_Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 630844\n",
      "Total chars: 52\n",
      "Number of sequences: 210268\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               92672     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 52)                6708      \n",
      "=================================================================\n",
      "Total params: 99,380\n",
      "Trainable params: 99,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base = LSTM_Generator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'fw&1}47z7\"o!13/0yszp&&3;,jy7x;54vzaf7/bj'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "base.predict()"
   ]
  }
 ]
}