{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "674dfd6ded4398e0679ff4f65e9a10a54ff0d14801bec0126172cfc3973d1cf1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, Activation\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('data_exercise/IMDB Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:int(len(data)*0.8)]\n",
    "test = df.iloc[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "print(len(data_train))\n",
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train.review)\n",
    "y_train = np.array(train.sentiment)\n",
    "X_test = np.array(test.review)\n",
    "y_test = np.array(test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_train = le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.5967 - accuracy: 0.7341 - val_loss: 0.5385 - val_accuracy: 0.8327\n",
      "Epoch 2/15\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5115 - accuracy: 0.8566 - val_loss: 0.5299 - val_accuracy: 0.8347\n",
      "Epoch 3/15\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4857 - accuracy: 0.8781 - val_loss: 0.5416 - val_accuracy: 0.8357\n",
      "Epoch 4/15\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4696 - accuracy: 0.8889 - val_loss: 0.5428 - val_accuracy: 0.8294\n",
      "Epoch 5/15\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4572 - accuracy: 0.8951 - val_loss: 0.5733 - val_accuracy: 0.8313\n",
      "Epoch 6/15\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4469 - accuracy: 0.9033 - val_loss: 0.5688 - val_accuracy: 0.8177\n",
      "Epoch 7/15\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4387 - accuracy: 0.9084 - val_loss: 0.5803 - val_accuracy: 0.8111\n",
      "Epoch 8/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4325 - accuracy: 0.9109 - val_loss: 0.6047 - val_accuracy: 0.8120\n",
      "Epoch 9/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4269 - accuracy: 0.9144 - val_loss: 0.6100 - val_accuracy: 0.8099\n",
      "Epoch 10/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4227 - accuracy: 0.9159 - val_loss: 0.6740 - val_accuracy: 0.8169\n",
      "Epoch 11/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4193 - accuracy: 0.9179 - val_loss: 0.6705 - val_accuracy: 0.8126\n",
      "Epoch 12/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4167 - accuracy: 0.9194 - val_loss: 0.7085 - val_accuracy: 0.8142\n",
      "Epoch 13/15\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4139 - accuracy: 0.9205 - val_loss: 0.6646 - val_accuracy: 0.8009\n",
      "Epoch 14/15\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4131 - accuracy: 0.9207 - val_loss: 0.7196 - val_accuracy: 0.8094\n",
      "Epoch 15/15\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4119 - accuracy: 0.9214 - val_loss: 0.7348 - val_accuracy: 0.8079\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167148d8448>"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.dtype = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "le.inverse_transform(prediction[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "le.inverse_transform(y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'First off I want to say that I lean liberal on the political scale and I found the movie offensive. I managed to watch the whole doggone disgrace of a film . This movie brings a low to original ideas. Yes it was original thus my 2 stars instead of 1. Are our film writers that uncreative that they can only come up with this?? Acting was horrible , and the characters were unlikeable for the most part. The lead lady in the story had no good qualities at all. They made her bf into some sort of a bad guy and I did not see that at all. Maybe I missed something , I do not know.He was the most down to earth, relevant character in the movie. I did not shell out any money for this garbage. I almost wish PETA would come to the rescue of this awful, offensive movie and form a protest. DISGUSTING thats all I have to say anymore !'"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_custom_model.tf')"
   ]
  },
  {
   "source": [
    "## La Forma Alternativa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_exercise/IMDB Dataset.csv')\n",
    "\n",
    "X_train, y_train = np.array(df.review), np.array(df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras más relevantes\n",
    "max_words = 3000\n",
    "\n",
    "# Se prepara el tokenizador y diccionario\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Se transforma X_train a un array de tokens\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in tf.keras.preprocessing.text.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# Matriz train_x modo binario\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "\n",
    "# Se transforma a array categórico y_train\n",
    "train_y = tf.keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el modelo\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, input_shape=(max_words,), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='sigmoid'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics= [\"CategoricalAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.3339 - categorical_accuracy: 0.8528 - val_loss: 0.2698 - val_categorical_accuracy: 0.8858\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.2491 - categorical_accuracy: 0.8967 - val_loss: 0.2724 - val_categorical_accuracy: 0.8852\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.1942 - categorical_accuracy: 0.9174 - val_loss: 0.2883 - val_categorical_accuracy: 0.8824\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.1252 - categorical_accuracy: 0.9480 - val_loss: 0.3426 - val_categorical_accuracy: 0.8830\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0762 - categorical_accuracy: 0.9684 - val_loss: 0.4237 - val_categorical_accuracy: 0.8828\n"
     ]
    }
   ],
   "source": [
    "hist_model = model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}