<!DOCTYPE html>
<!-- saved from url=(0070)https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/ -->
<html lang="en-us" class="js-focus-visible" data-js-focus-visible=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">  <link rel="alternate" type="application/atom+xml" href="https://vgpena.github.io/feed.xml" title="Like, subscribe, &amp; download my mixtape"> <meta property="og:type" content="website"> <meta property="og:url" content="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/"> <meta property="og:site_name" content="hey it&#39;s violet"> <meta property="og:title" content="Classifying Tweets with Keras and TensorFlow"> <meta property="og:description" content="I had a week to make my first neural network. I dove into TensorFlow and Keras, and came out with a deep neural network, trained on tweets, that can classify text sentiment. Here&#39;s an introduction to neural networks and machine learning, and step-by-step instructions of how to do it yourself."> <meta property="og:image" content="https://vgpena.github.io/images/keras/noice.gif"> <meta name="twitter:domain" value="vgpena.github.io"> <meta property="twitter:title" content="Classifying Tweets with Keras and TensorFlow"> <meta property="twitter:description" ontent="I had a week to make my first neural network. I dove into TensorFlow and Keras, and came out with a deep neural network, trained on tweets, that can classify text sentiment. Here&#39;s an introduction to neural networks and machine learning, and step-by-step instructions of how to do it yourself."> <meta property="twitter:url" content="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/"> <meta property="twitter:image" content="https://vgpena.github.io/images/keras/noice.gif"> <meta name="author" content="Violet Pe√±a"> <meta name="description" content="I had a week to make my first neural network. I dove into TensorFlow and Keras, and came out with a deep neural network, trained on tweets, that can classify text sentiment. Here&#39;s an introduction to neural networks and machine learning, and step-by-step instructions of how to do it yourself."> <meta name="viewport" content="width=device-width, initial-scale=1"> <script>
  if ('FontFace' in window) {
    const fonts = [
      {
        name: 'Overpass Mono',
        files: [
          {
            fileName: 'OverpassMono-Light',
            weight: 'normal',
            style: 'normal',
          },
          {
            fileName: 'OverpassMono-Bold',
            weight: 'bold',
            style: 'normal',
          }
        ]
      },
      {
        name: 'Pitch Display',
        files: [
          {
            fileName: 'Pitch-Display',
            weight: 'normal',
            style: 'normal',
          }
        ]
      },
      {
        name: 'Karla',
        files: [
          {
            fileName: 'Karla-Regular',
            weight: 'normal',
            style: 'normal',
          },
          {
            fileName: 'Karla-Bold',
            weight: 'bold',
            style: 'normal',
          },
          {
            fileName: 'Karla-Italic',
            weight: 'normal',
            style: 'italic',
          },
          {
            fileName: 'Karla-BoldItalic',
            weight: 'bold',
            style: 'italic',
          },
        ]
      },
    ];

    fonts.forEach((font) => {
      font.files.forEach((file) => {
        const currFont = new FontFace(
          font.name,
          `url(/fonts/${ file.fileName }.woff2), url(/fonts/${ file.fileName }.woff)`,
          {
            weight: file.weight,
            style: file.style,
          }
        );

        currFont.load().then(() => {
          document.fonts.add(currFont);
        });
      });
    });
  }
</script> <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-92096829-1', 'auto');
      ga('send', 'pageview');
    </script> <script async="" src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/analytics.js.descarga"></script> <title>Classifying Tweets with Keras and TensorFlow | hey it's violet</title> <link href="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/styles.css" rel="stylesheet" media="screen"> <style type="text/css">
@font-face {
  font-weight: 400;
  font-style:  normal;
  font-family: 'Circular-Loom';

  src: url('https://cdn.loom.com/assets/fonts/circular/CircularXXWeb-Book-cd7d2bcec649b1243839a15d5eb8f0a3.woff2') format('woff2');
}

@font-face {
  font-weight: 500;
  font-style:  normal;
  font-family: 'Circular-Loom';

  src: url('https://cdn.loom.com/assets/fonts/circular/CircularXXWeb-Medium-d74eac43c78bd5852478998ce63dceb3.woff2') format('woff2');
}

@font-face {
  font-weight: 700;
  font-style:  normal;
  font-family: 'Circular-Loom';

  src: url('https://cdn.loom.com/assets/fonts/circular/CircularXXWeb-Bold-83b8ceaf77f49c7cffa44107561909e4.woff2') format('woff2');
}

@font-face {
  font-weight: 900;
  font-style:  normal;
  font-family: 'Circular-Loom';

  src: url('https://cdn.loom.com/assets/fonts/circular/CircularXXWeb-Black-bf067ecb8aa777ceb6df7d72226febca.woff2') format('woff2');
}</style></head> <body data-new-gr-c-s-check-loaded="14.1018.0" data-gr-ext-installed=""> <header id="site-header" role="banner" aria-label="Hey it&#39;s Violet: a blog"> <h1 id="site-title"> <a href="https://vgpena.github.io/" aria-label="Back to home">Hey it's Violet</a> </h1> </header> <main> <article aria-labelledby="article-title"> <div id="article-heading" role="heading"> <h1 id="article-title"> Classifying Tweets with Keras and TensorFlow </h1> <div id="article-info"> <span class="article-info-piece" id="article-subtitle"> In case you can't tell when people are upset on the internet </span> <span class="article-info-piece" id="article-date" aria-label="Published September 02, 2017"> 9.2.2017 </span> </div> </div> <div class="content"> <p>Summer is drawing to a close. The air is humid and still. You‚Äôre between projects at work. What do you do with these few empty days?</p> <p>If you‚Äôre like me, you train a neural net. It had been on my ‚ÄúTo Do‚Äù list for about a year now, and while I had done some reading and tutorials, I hadn‚Äôt yet made my own from the ground up.</p> <p>I spent a few days chasing dead-ish ends, doing even more tutorials, and tinkering. I emerged with a custom neural net that could classify text as positive or negative with 79.3% accuracy.</p> <p>Here‚Äôs how to create your own neural net using Python, TensorFlow, and Keras! Happy learnings ‚ú®</p> <h2 class="section-title" id="so-what-do-neural-nets-do"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#so-what-do-neural-nets-do" class="section-inner">So what do neural nets do?</a></h2> <p>Neural net[work]s are collections of nodes that apply transformations to data. Their core behavior is: given an input, generate an output.</p> <p>The intriguing aspect of neural nets is that we don‚Äôt tell them <em>how</em> to generate that output. Rather, we set them up to ‚Äúlearn‚Äù how to generate outputs based on massive amounts of training data. Training data consists of an input and an output, usually labelled by humans. The neural net intakes a piece of training data, generates an output, compares that output to the actual result, and adjusts the weights on its nodes ‚Äî that is, how likely an individual node is to return a certain intermediate value. Modifying these weights leads a net to return one value or another given an input, and is how the net refines its accuracy as it trains.</p> <p></p><figure role="img" class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/nnet.png" alt="A diagram of a neural network with two hidden layers in addition to input and output layers." title="A sample neural network." width="597px" height="324px"> <figcaption> <p>A neural network with two hidden layers. <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Source</a></p> </figcaption> </figure><p></p> <p>Nodes are arranged in layers within the neural net. All neural nets have an input layer and an output layer; within, they have at least one additional ‚Äúhidden‚Äù layer. ‚ÄúDeep‚Äù neural nets have more than one hidden layer. This is also the difference, name-wise, between ‚Äúmachine‚Äù and ‚Äúdeep‚Äù learning. Once you train a neural net, it contains a fairly accurate self-adjusted system for creating outputs. Ideally, you can feed novel data into the net and end up with a meaningful output.</p> <p>Neural nets can either <em>classify</em> extant data or <em>predict</em> new data. <a href="https://www.technologyreview.com/s/419223/using-neural-networks-to-classify-music/">Identifying musical genre</a> is an example of the former; <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html">fusing visual styles</a>, of the latter. Kristen Stewart -- yes, the lead from <em>Twilight</em> -- used this to give her film <em>Come Swim</em> (2017) an impressionist look. She even <a href="https://arxiv.org/pdf/1701.04928v1.pdf">co-authored a paper</a> about the technique üî•üî•üî•</p> <p></p><figure role="img" class="image-wrap image-primary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/come-swim.png" alt="Four squares illustrating different degrees of visual treatment for &quot;Come Swim&quot;. All depict a man in water, but each has been modified to have its own visual style." title="Details from &quot;Come Swim&quot;" width="1404px" height="797px"> <figcaption> <p>Fine-tuning the visual treatment for <em>Come Swim</em>. <a href="https://arxiv.org/pdf/1701.04928v1.pdf">Source</a></p> </figcaption> </figure><p></p> <p><a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">Deep Dream</a>, a project out of Google, both classifies and predicts. It uses classifications to suggest predictions, which in turn amplify the certainty of classifications. It turns landscapes and portraits into fields of roiling curves often resembling human eyes.</p> <p></p><figure role="img" class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/deep-dream.jpg" alt="&quot;The Scream&quot; by Edvard Munch, but all the fields of color have been subdivided into roiling curls resembling nested eyes, cars, and dogs." title="A sample Deep Dream project" width="767px" height="965px"> <figcaption> <p>A sample of what Deep Dream can do when applied to Edvard Munch's <em>The Scream</em>. <a href="https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ/photo/AF1QipPAcXwHkI3k8Sqnq-3WJUfXCZR68rZYTiR2b3te?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB">Source</a></p> </figcaption> </figure><p></p> <hr> <p>I won‚Äôt get into the specifics of neural nets or training/adjustment mechanics here. I‚Äôve done some reading but I have a long way to go before I am truly 1337. I highly recommend <em><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></em> if you‚Äôre looking for a great intro to the field.</p> <h2 class="section-title" id="whats-emourem-neural-net-going-to-do"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#whats-emourem-neural-net-going-to-do" class="section-inner">What‚Äôs <em>our</em> neural net going to do?</a></h2> <p>We‚Äôll perform the relatively straightforward task of classifying text ‚Äî specifically, we‚Äôll predict whether text expresses a positive or a negative sentiment. As training data, we‚Äôre using the behemoth <a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/">Twitter Sentiment Analysis Dataset</a> documented at ThinkNook.</p> <h2 class="section-title" id="getting-started-environment-and-tools"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#getting-started-environment-and-tools" class="section-inner">Getting started: environment and tools</a></h2> <p>Our neural net is Python 2 based, so make sure that‚Äôs what you‚Äôre working with on your own machine. I highly recommend <a href="https://virtualenv.pypa.io/en/stable/">virtualenv</a> for managing your package installs, and <a href="https://ipython.org/">IPython</a> as an interactive editor. The net itself will be built using <a href="https://www.tensorflow.org/">TensorFlow</a>, an open-source, Google-backed machine learning framework. We‚Äôre laying <a href="https://keras.io/">Keras</a> on top of TensorFlow to act as an API and simplify TensorFlow‚Äôs syntax. If you want to dig into TensorFlow on its own for a bit, <a href="https://www.tensorflow.org/get_started/mnist/beginners">their ‚ÄúFor Beginners‚Äù tutorial</a> is informative and surprisingly painless.</p> <h2 class="section-title" id="language-and-machines"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#language-and-machines" class="section-inner">Language and machines</a></h2> <p>For me, there are few joys on par with working with natural language. In machine learning, we have two ways of representing language language: vector embeddings or one-hot matrices. Vector embeddings are spatial mappings of words or phrases. Relative locations of words indicate similarity and suggest semantic relationships ‚Äî for instance, <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">vector embeddings can be used to generate analogies</a>.</p> <p></p><figure role="img" class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/vectors.png" alt="Graphs depict vector embeddings -- circles with lines drawn between them to convey analogous relationships between the words represented by the circles." title="A sample neural network." width="1576px" height="772px"> <figcaption> <p>Sample vector embeddings that demonstrate analogous relationships between words. <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Source</a></p> </figcaption> </figure><p></p> <p>One-hot matrices, on the other hand, contain no linguistic information. (If you‚Äôre taking all the recommended detours, you‚Äôll remember one-hot matrices from the <a href="https://www.tensorflow.org/get_started/mnist/beginners">TensorFlow MNIST digit classification tutorial</a>.) They‚Äôre na√Øve; they indicate what data they contain but suggest nothing <em>about</em> that data, or its relationship to other information. I decided to use one-hot matrices so that I could focus on other aspects of the other project. So let‚Äôs talk about them for a bit.</p> <h2 class="section-title" id="enter-the-one-hot-matrix"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#enter-the-one-hot-matrix" class="section-inner">Enter the (one-hot) matrix</a></h2> <p>One-hot matrices are called ‚Äúone-hot‚Äù because they each embody one dimension of difference from each other; each matrix has one distinguishing (‚Äúhot‚Äù) characteristic. We can take all the data in our system and represent them using this flattened system. As an example, let‚Äôs take a couple of lines from <a href="https://www.python.org/dev/peps/pep-0020/">PEP 20</a>:</p> <div class="highlight"><pre class="code plaintext"><code>Complex is better than complicated.
Flat is better than nested.
</code></pre></div> <p>How do we represent that in a one-hot matrix?</p> <p>We begin by tokenizing the utterance; that is, breaking it into words. We end up with</p> <div class="highlight"><pre class="code python"><code><span class="p">[</span><span class="s">'complex'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'better'</span><span class="p">,</span> <span class="s">'than'</span><span class="p">,</span> <span class="s">'complicated'</span><span class="p">,</span> <span class="s">'flat'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'better'</span><span class="p">,</span> <span class="s">'than'</span><span class="p">,</span> <span class="s">'nested'</span><span class="p">]</span>
</code></pre></div> <p>Now we can create a lookup dictionary of all the unique words. We now have:</p> <div class="highlight"><pre class="code python"><code><span class="p">{</span>
  <span class="s">'complex'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s">'is'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'better'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s">'than'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
  <span class="s">'complicated'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
  <span class="s">'flat'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
  <span class="s">'nested'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div> <p>Count doesn‚Äôt matter; order doesn‚Äôt matter. Each token just needs a unique identifier. Now to create the matrices: each token needs to be transformed from a string into an array. Each array is of the length of the dictionary, and each value in the dictionary that‚Äôs <em>not</em> the value of the current token is represented by a 0. The value of the token is represented with a 1. ‚ÄúComplex is better than complicated‚Äù would look like:</p> <div class="highlight"><pre class="code python"><code><span class="p">[</span>
  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1">#complex
</span>  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1">#is
</span>  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1">#better
</span>  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1">#than
</span>  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="c1">#complicated
</span><span class="p">]</span>
</code></pre></div> <p>Now we can deal with the tokens in a uniform way, since they‚Äôre all represented by isomorphic data structures, and once we‚Äôre done we can look up their values using the dictionary we made earlier.</p> <p>One-hot matrices get large quickly. In my example, I‚Äôm ‚Äúonly‚Äù using the top 3000 most-commonly occurring words in the training corpus. This means that each word becomes represented by an array 3000 items long üò¨ Whether using one-hot matrices or not, we‚Äôre reckoning with a ton of data, but one-hot matrices are ideally used for a small or finite dataset. In MNIST, for example, a one-hot matrix is used to encode information about whether an image represents a digit from 0 to 9. All the arrays are kept nice and small to a length of 10.</p> <h2 class="section-title" id="lets-get-cooking"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#lets-get-cooking" class="section-inner">Let‚Äôs get cooking</a></h2> <p>Enough preamble; time to get started actually building our neural net! Our work will be based on <a href="https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py">the Reuters example</a> in the Keras github repo, but we‚Äôll use our own data set and make a couple more tweaks on the way.</p> <p>I will be going over all the code in detail, but <a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023">I have published it in full in a gist</a>. It doesn't have a ton of backstory but it does have all the code in one nice place for you.</p> <hr> <p>Neural nets can take anywhere from a few moments to days to train, depending on your hardware and on how large and/or complex your dataset is. This net took me ~60 minutes to train on a mid-2015 MacBook Pro (and it got NOISY üòÖ ). My point is, you probably won‚Äôt want to have to train the net every single time you want to use it. The last step of our training script will also save the net so that we can ‚Äúboot it up‚Äù quickly from another script when we actually want to consult it.</p> <p>Let‚Äôs worry about the training script first. We‚Äôll need to:</p> <ol> <li>Get our data into a usable format</li> <li>Build our neural net</li> <li>Train it with said data</li> <li>Save the neural net for future use</li> </ol> <h2 class="section-title" id="organizing-our-data"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#organizing-our-data" class="section-inner">Organizing our data</a></h2> <p>We‚Äôre using the <a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/">Twitter Sentiment Analysis Dataset</a> available via ThinkNook. Be prepared; this dataset is <em>extremely large</em> and may take forever-ish to open in Excel (I had more success with Numbers).</p> <p>One you open it, you can see a massive table that starts with this:</p> <div class="table-wrap"><table><tbody><tr> <th><strong>ItemID</strong></th> <th><strong>Sentiment</strong></th> <th><strong>SentimentSource</strong></th> <th><strong>SentimentText</strong></th> </tr> <tr> <td><strong>1</strong></td> <td>0</td> <td>Sentiment140</td> <td>is so sad for my APL friend.............</td> </tr> <tr> <td><strong>2</strong></td> <td>0</td> <td>Sentiment140</td> <td>I missed the New Moon trailer...</td> </tr> <tr> <td><strong>3</strong></td> <td>1</td> <td>Sentiment140</td> <td>omg its already 7:30 :O</td> </tr> </tbody></table></div> <p>The only columns we‚Äôre interested in here are 1 and 3 ‚Äî we‚Äôll be training our net on inputs of column <code>SentimentText</code> with outputs of <code>Sentiment</code>.</p> <p>We need to convert <code>SentimentText</code> utterances to one-hot matrices, and create a dictionary of all the words we keep track of. Here, the <code>numpy</code> library is your friend. I hadn‚Äôt used it much before this but it‚Äôs super powerful and has some really useful built-in utilities.</p> <div class="highlight"><pre class="code python"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># extract data from a csv
# notice the cool options to skip lines at the beginning
# and to only take data from certain columns
</span><span class="n">training</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">'path/to/your/data.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">','</span><span class="p">,</span> <span class="n">skip_header</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c1"># create our training data from the tweets
</span><span class="n">train_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">training</span><span class="p">]</span>
<span class="c1"># index all the sentiment labels
</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">training</span><span class="p">])</span>
</code></pre></div> <p>Okay, we‚Äôve indexed all of our data; time to use Keras to make it machine-friendly.</p> <div class="highlight"><pre class="code python"><code><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">keras.preprocessing.text</span> <span class="k">as</span> <span class="n">kpt</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="c1"># only work with the 3000 most popular words found in our dataset
</span><span class="n">max_words</span> <span class="o">=</span> <span class="mi">3000</span>

<span class="c1"># create a new Tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_words</span><span class="p">)</span>
<span class="c1"># feed our tweets to the Tokenizer
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

<span class="c1"># Tokenizers come with a convenient list of words and IDs
</span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
<span class="c1"># Let's save this out so we can use it later
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'dictionary.json'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">dictionary_file</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">dictionary_file</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">convert_text_to_index_array</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># one really important thing that `text_to_word_sequence` does
</span>    <span class="c1"># is make all texts the same length -- in this case, the length
</span>    <span class="c1"># of the longest text in the set.
</span>    <span class="k">return</span> <span class="p">[</span><span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">kpt</span><span class="p">.</span><span class="n">text_to_word_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>

<span class="n">allWordIndices</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># for each tweet, change each token to its ID in the Tokenizer's word_index
</span><span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">train_x</span><span class="p">:</span>
    <span class="n">wordIndices</span> <span class="o">=</span> <span class="n">convert_text_to_index_array</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">allWordIndices</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wordIndices</span><span class="p">)</span>

<span class="c1"># now we have a list of all tweets converted to index arrays.
# cast as an array for future usage.
</span><span class="n">allWordIndices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">allWordIndices</span><span class="p">)</span>

<span class="c1"># create one-hot matrices out of the indexed tweets
</span><span class="n">train_x</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">sequences_to_matrix</span><span class="p">(</span><span class="n">allWordIndices</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
<span class="c1"># treat the labels as categories
</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">train_y</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div> <p>‚ú® Cooooooooooooool. ‚ú® Now you have training data and labels that you‚Äôll be able to pipe right into your neural net.</p> <h2 class="section-title" id="making-a-model"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#making-a-model" class="section-inner">Making a model</a></h2> <p>Keras makes building neural nets as simple as possible, to the point where you can add a layer to the network in short line of code. Here‚Äôs how I built my net:</p> <div class="highlight"><pre class="code python"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_words</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</code></pre></div> <p>Looks simple enough. What does it mean?? üåàüåà</p> <p>Keras‚Äô <code>Sequential()</code> is a simple type of neural net that consists of a ‚Äústack‚Äù of layers executed in order.</p> <p>If we wanted to, we could make a stack of only two layers (input and output) to make a complete neural net ‚Äî without hidden layers, it wouldn‚Äôt be considered a deep neural net.</p> <p>The input and output layers are the most important, since they determine the overall shape of the neural net. You need to know what kind of input to expect, and what kind of output you want.</p> <p></p><figure role="img" class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/nnet2.png" alt="A diagram of a neural net used to identify digits using MNIST data." title="A page from the Perspectiva Corporum Regularium" width="537px" height="447px"> <figcaption> <p>A representation of a neural net for identifying digits using the MNIST dataset. <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Source</a></p> </figcaption> </figure><p></p> <p>Out network will mostly consist of <code>Dense</code> layers ‚Äî the ‚Äústandard‚Äù, linear neural net layer of inputs, weights, and outputs.</p> <p>In our case, we‚Äôre inputting a sentence that will be converted to a one-hot matrix of length <code>max_words</code> ‚Äî here, 3000. We also include how many outputs we want to come out of that layer (512, for funsies) and what kind of maximization (or ‚Äúactivation‚Äù) function to use.</p> <p>Activation functions are used when training the network; they tell the network how to judge when a weight for a particular node has created a good fit. In the first layer, I use <code>relu</code> (also for funsies). Activation functions differ, mostly in speed, but all the ones available in Keras and TensorFlow are viable; feel free to play around with them. If you don‚Äôt explicitly add an activation function, that layer will use a linear one.</p> <p>Our output layer consists of two possible outputs, since that‚Äôs how many categories our data could get sorted into. If you use a neural net to predict rather than classify, you‚Äôre actually creating a neural net with one possible output ‚Äî the prediction.</p> <p>In between the input and output layers, we have one more <code>Dense</code> layer and two <code>Dropout</code> layers. Dropouts are used to randomly remove data, which can help avoid overfitting. Overfitting can happen when you keep training on the same or overly-similar data ‚Äî as you train, your accuracy will hold steady or drop instead of rising.</p> <p>As the last step before training, we need to compile the network:</p> <div class="highlight"><pre class="code python"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
  <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div> <p>Specifying that we want to collect the <code>accuracy</code> metric will give us really helpful live output as we train our model.</p> <h2 class="section-title" id="how-to-train-your-network"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#how-to-train-your-network" class="section-inner">How to train your network</a></h2> <p>This is some tiny code that will take a while to run:</p> <div class="highlight"><pre class="code python"><code><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
  <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> <p>We‚Äôre fitting (training) our model off of inputs <code>train_x</code> and categories <code>train_y</code>. We evaluate data in groups of <code>batch_size</code>, checking the network‚Äôs accuracy, tweaking node weights, and then running through another batch. Small batches let you train networks much more quickly than if you tried to use a batch the size of your entire training dataset.</p> <p><code>epochs</code> is how many times you do this batch-by-batch splitting. I‚Äôve found 5 to be good in this case; I tried 7, but ended up overfitting.</p> <p><code>validation_split</code> says how much of your input you want to be reserved for testing data ‚Äî essential for seeing how accurate your network is at that point. Recommended training-to-test ratios are 80:20 or 90:10. You don‚Äôt want to compromise the size of your training corpus, but you need enough test data to actually see how your net is doing.</p> <p>Now go get coffee, or maybe a meal. And if you‚Äôre on a laptop, make sure it‚Äôs plugged in ‚Äî training can be a real battery killer ‚ò†Ô∏è</p> <p></p><figure role="img" class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/perspectiva.jpg" alt="An etching of a fractured geometric figure propped up on a platform surrounded by quare pyramids and a crucifix." title="A page from the Perspectiva Corporum Regularium" width="1191px" height="800px"> <figcaption> <p>If you need something to lose yourself in for about an hour, I suggest the <em><a href="http://digital.slub-dresden.de/werkansicht/dlf/12830/">Perspectiva Corporum Regularium</a></em>.</p> </figcaption> </figure><p></p> <p>As you train the neural net, Keras will output running stats on what epoch you‚Äôre in, how much time is left in that epoch of training, and current accuracy. The value to watch is not <code>acc</code> but <code>val_acc</code>, or Validation Accuracy. This is your neural net's score when predicting values for data in your validation split.</p> <p>Your accuracy should start out low per epoch and rise throughout the epoch; it should increase at least a little across epochs. If your accuracy starts decreasing, you‚Äôre overfitting.</p> <p>This is some sample output from training using this code over five epochs:</p> <div class="highlight"><pre class="code python"><code><span class="mi">1420764</span><span class="o">/</span><span class="mi">1420764</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">780</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4947</span> <span class="o">-</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.7610</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.4500</span> <span class="o">-</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.7884</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1420764</span><span class="o">/</span><span class="mi">1420764</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">850</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4737</span> <span class="o">-</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.7760</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.4481</span> <span class="o">-</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.7902</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1420764</span><span class="o">/</span><span class="mi">1420764</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">788</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4662</span> <span class="o">-</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.7817</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.4446</span> <span class="o">-</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.7921</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1420764</span><span class="o">/</span><span class="mi">1420764</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">819</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4607</span> <span class="o">-</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.7859</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.4471</span> <span class="o">-</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.7921</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">1420764</span><span class="o">/</span><span class="mi">1420764</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">829</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.4569</span> <span class="o">-</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.7887</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.4439</span> <span class="o">-</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.7927</span>
</code></pre></div> <p>Our accuracy increases from 78.8% accurate by the end of Epoch 1 to 79.3% at the end of Epoch 5.</p> <p></p><div class="image-wrap image-primary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/noice.gif" alt="NOICE!" title="NOICE!!" width="500px" height="201px"> </div><p></p> <h2 class="section-title" id="saving-your-model"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#saving-your-model" class="section-inner">Saving your model</a></h2> <p>Once you‚Äôre done training, it‚Äôs time to save your net so that you don‚Äôt have to keep repeating all of those steps.</p> <p>Your model gets saved in two parts: One is the model‚Äôs structure itself; the other is the weights used in those model‚Äôs nodes.</p> <div class="highlight"><pre class="code python"><code><span class="n">model_json</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to_json</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'model.json'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">json_file</span><span class="p">:</span>
    <span class="n">json_file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">model_json</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s">'model.h5'</span><span class="p">)</span>
</code></pre></div> <p>And that‚Äôs it!</p> <h2 class="section-title" id="party-time"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#party-time" class="section-inner">Party time</a></h2> <p>Finally you can use your neural net! In a new file, you‚Äôll open the model, its weights, and your dictionary, and then put those together to classify text. We‚Äôre going to go over the whole file at once because I‚Äôm excited to actually use it:</p> <div class="highlight"><pre class="code python"><code><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">keras.preprocessing.text</span> <span class="k">as</span> <span class="n">kpt</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">model_from_json</span>

<span class="c1"># we're still going to use a Tokenizer here, but we don't need to fit it
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="c1"># for human-friendly printing
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'negative'</span><span class="p">,</span> <span class="s">'positive'</span><span class="p">]</span>

<span class="c1"># read in our saved dictionary
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'dictionary.json'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">dictionary_file</span><span class="p">:</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">dictionary_file</span><span class="p">)</span>

<span class="c1"># this utility makes sure that all the words in your input
# are registered in the dictionary
# before trying to turn them into a matrix.
</span><span class="k">def</span> <span class="nf">convert_text_to_index_array</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">kpt</span><span class="p">.</span><span class="n">text_to_word_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">wordIndices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">wordIndices</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"'%s' not in training corpus; ignoring."</span> <span class="o">%</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wordIndices</span>

<span class="c1"># read in your saved model structure
</span><span class="n">json_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'model.json'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">loaded_model_json</span> <span class="o">=</span> <span class="n">json_file</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">json_file</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
<span class="c1"># and create a model from that
</span><span class="n">model</span> <span class="o">=</span> <span class="n">model_from_json</span><span class="p">(</span><span class="n">loaded_model_json</span><span class="p">)</span>
<span class="c1"># and weight your nodes with your saved values
</span><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'model.h5'</span><span class="p">)</span>

<span class="c1"># okay here's the interactive part
</span><span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">evalSentence</span> <span class="o">=</span> <span class="nb">raw_input</span><span class="p">(</span><span class="s">'Input a sentence to be evaluated, or Enter to quit: '</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">evalSentence</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># format your input for the neural net
</span>    <span class="n">testArr</span> <span class="o">=</span> <span class="n">convert_text_to_index_array</span><span class="p">(</span><span class="n">evalSentence</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">sequences_to_matrix</span><span class="p">([</span><span class="n">testArr</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
    <span class="c1"># predict which bucket your input belongs in
</span>    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="c1"># and print it for the humons
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"%s sentiment; %f%% confidence"</span> <span class="o">%</span> <span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)],</span> <span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre></div> <p>If you run this file, you create a new model using the saved structure, and then you get a little command prompt for input to classify.</p> <p><code>model.predict()</code> takes what you give it, runs it through the trained neural net, and gives you a reading of how confident it is that that input belongs in each output bucket. In our case, we have two outputs, so we have two confidence estimations that range from 0 to 1; whichever one is higher is the network‚Äôs ultimate classification of that data.</p> <div class="highlight"><pre class="code plaintext"><code>Input a sentence to be evaluated, or Enter to quit: It's alive! :D
positive sentiment; 80.760884% confidence
</code></pre></div> <p>üëåüèª</p> <h2 class="section-title" id="you-did-it"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#you-did-it" class="section-inner">You did it!</a></h2> <p>You‚Äôve trained your first neural net that evaluates text as expressing positive or negative sentiment. And it works well(ish):</p> <div class="highlight"><pre class="code plaintext"><code>Input a sentence to be evaluated, or Enter to quit: That went better than expected
positive sentiment; 56.355631% confidence
</code></pre></div><div class="highlight"><pre class="code plaintext"><code>Input a sentence to be evaluated, or Enter to quit: That did not go as expected
negative sentiment; 86.936867% confidence
</code></pre></div><h2 class="section-title" id="next-steps"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#next-steps" class="section-inner">Next steps</a></h2> <p></p><div class="image-wrap image-secondary"> <img src="./Classifying Tweets with Keras and TensorFlow _ hey it&#39;s violet_files/party.gif" alt="" title="Thanks for sticking with me through all of that." width="480px" height="360px"> </div><p></p> <p>This is a really basic neural net, and there‚Äôs a lot more I‚Äôd like to investigate. Among other things:</p> <ul> <li>You can deploy machine learning models to the cloud using <a href="https://cloud.google.com/ml-engine/">Google Cloud ML</a> ‚Äî what could I do if I ported my local machine to operate on Google Cloud Platform, or if I started building new models using that service?</li> <li>I chose the option of less-powerful word indexing by using one-hot matrices instead of vector embeddings. What accuracy could I get if I started using the latter?</li> <li>Classification is great but I‚Äôd really love to work on text <em>generation</em>, guessing at the next most-likely word in the sequence.</li> </ul> <p>All of the code in this post, plus the requirements file, is up <a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023">in a Github gist</a>.</p> <p>Happy coding! And thanks for following me on this journey üåü</p> <h2 class="section-title" id="coda-how-well-does-it-work-and-more-on-data"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#coda-how-well-does-it-work-and-more-on-data" class="section-inner">Coda: How well does it work? And more on data</a></h2> <p>I‚Äôm super proud of this neural net but 79% accuracy means that it‚Äôs wrong 21% of the time.</p> <div class="highlight"><pre class="code plaintext"><code>Input a sentence to be evaluated, or Enter to quit: I wish I could show you when you are lonely or in darkness the astonishing light of your own being
negative sentiment; 87.021768% confidence
</code></pre></div><div class="highlight"><pre class="code plaintext"><code>Input a sentence to be evaluated, or Enter to quit: foo bar
positive sentiment; 62.751633% confidence
</code></pre></div> <p>ü§î ü§î Those don‚Äôt look quite right.</p> <p>We can always build better models, but a lot of this is <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out">Garbage In, Garbage Out</a>. The dataset I used is incredibly large, but looking through it, there are classifications I don‚Äôt agree with, such as marking <code>... health class (what a joke!)</code> as positive. Without bringing compensation algorithms into the mix, your neural net can only be as accurate as your training data.</p> <hr> <p>On that note, I encourage you to examine your training data closely. What biases or prejudices might have influenced that information? Those biases will also be present, explicitly or implicitly, in your output.</p> <blockquote> <p>The past is a very racist place. And we only have data from the past to train Artificial Intelligence. <cite> Trevor Paglen</cite></p>  </blockquote> <p>This doesn‚Äôt mean that any and all data are inherently biased and therefore unusable ‚Äî we just need to be aware of that bias and work to eradicate it. Researchers from Boston University and Microsoft have <a href="https://arxiv.org/abs/1607.06520">created ways to work with appropriately gendered analogies without reinforcing gender stereotypes</a>. <a href="https://artificialintelligencenow.com/">AI Now</a> is an entire research organization dedicated to teasing out the biases and impacts of artificial intelligence and machine learning.</p> <p>ML is powerful and we can make amazing things with it. Let‚Äôs use it to create a more equitable future.</p> <h2 class="section-title" id="downloads"><a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#downloads" class="section-inner">Downloads</a></h2> <ul> <li><a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023">code</a></li> <li><a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/">dataset</a></li> </ul> </div> </article> </main> <footer> <nav id="footer-nav" role="navigation" aria-label="Navigate to Other Pages" class="footer-item noprint"> <ul id="footer-nav-links"> <li class="footer-nav-link"> <a href="https://vgpena.github.io/" class="footer-unit">Index</a> </li> <li class="footer-nav-link"> <a href="https://vgpena.github.io/info" class="footer-unit">Info</a> </li> <li class="footer-nav-link"> <a href="https://vgpena.github.io/feed.xml" class="footer-unit">RSS</a> </li> <li class="footer-nav-link"> <a href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/#" class="footer-unit">Top</a> </li> </ul> </nav> <div id="footer-colophon" aria-label="Made with love in 2021" class="footer-item footer-unit noprint"> <div> ‚ô• 2021 </div> <div class="footer-disclaimer">All opinions are my own.</div> </div> <div class="printonly aux"> http://violet.is || Violet Pe√±a || 2021 </div> </footer>  </body><loom-container id="lo-engage-ext-container"><div></div><loom-shadow classname="resolved"></loom-shadow></loom-container></html>